{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kenya Constitution AI Agent – Business Understanding\n",
    "\n",
    "## Project Context\n",
    "The Kenyan Constitution of 2010 is a comprehensive legal document that governs the country's laws and citizens' rights. However, it is long, complex, and written in legal language that can be difficult for the general population, especially youths, to understand. \n",
    "\n",
    "Accessing relevant information quickly can be challenging, making it difficult for citizens to exercise their rights or comply with legal obligations.\n",
    "\n",
    "## Business Problem\n",
    "*Problem Statement:*  \n",
    "Citizens, especially young people, need an accessible way to understand and query the Kenyan Constitution in everyday language. Traditional legal consultation is expensive, and reading the full document is time-consuming.\n",
    "\n",
    "*Objective:*  \n",
    "Develop an AI agent that uses Natural Language Processing (NLP) to understand user queries and provide relevant answers from the Constitution. The agent should:\n",
    "\n",
    "- Accept questions in *English and Kiswahili*.\n",
    "- Return accurate, understandable answers.\n",
    "- Quote the specific article/section from the Constitution for credibility.\n",
    "\n",
    "*Stakeholders:*  \n",
    "- Kenyan youths (primary users)\n",
    "- NGOs and civic education organizations\n",
    "- Government institutions promoting civic engagement\n",
    "\n",
    "## Expected Impact\n",
    "- *Empowerment:* Citizens can better understand their rights and duties.\n",
    "- *Accessibility:* Reduces reliance on legal experts for basic constitutional questions.\n",
    "- *Scalability:* The system can be deployed online and integrated via API for wider use.\n",
    "\n",
    "## Key Considerations\n",
    "- Language support (English + Kiswahili)\n",
    "- Handling legal jargon\n",
    "- Accurate referencing of articles/chapters\n",
    "- Efficient query handling for fast responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data Understanding\n",
    "\n",
    "## Overview of the Data\n",
    "For this project, our data sources are:\n",
    "\n",
    "1. *English version of the Kenyan Constitution (2010)*  \n",
    "   - Source: [The Constitution of Kenya 2010 PDF](Data/The_Constitution_of_Kenya_2010.pdf)  \n",
    "   - Contains all chapters, articles, and legal provisions in English.  \n",
    "   - Needs text extraction and cleaning to convert it into a structured format for NLP.\n",
    "\n",
    "2. *Kiswahili version of the Kenyan Constitution*  \n",
    "   - Source: [Kielelezo_Pantanifu_cha_Katiba_ya_Kenya.pdf](Data/Kielelezo_Pantanifu_cha_Katiba_ya_Kenya.pdf)  \n",
    "   - Same content as the English version, translated into Kiswahili.  \n",
    "   - Requires extraction, cleaning, and alignment with the English dataset.\n",
    "\n",
    "*Purpose of Using Both Languages:*  \n",
    "- Enable the AI agent to respond to user queries in *English* or *Kiswahili*.  \n",
    "- Improve accessibility and inclusivity for all Kenyan citizens.  \n",
    "\n",
    "## Data Structure\n",
    "After preprocessing, the expected data structure is:\n",
    "\n",
    "| Article/Section | Text (English) | Text (Kiswahili) |\n",
    "|-----------------|----------------|-----------------|\n",
    "| Article 1       | Text content   | Swahili content |\n",
    "| Article 2       | Text content   | Swahili content |\n",
    "| ...             | ...            | ...             |\n",
    "\n",
    "*Notes:*  \n",
    "- Each row corresponds to an *article* or *clause*.  \n",
    "- This will allow the NLP model to retrieve relevant sections when users ask questions.  \n",
    "\n",
    "## Data Quality Considerations\n",
    "- PDFs contain headers, footers, and formatting that need cleaning.  \n",
    "- Ensure *text alignment* between English and Kiswahili versions.  \n",
    "- Maintain *article/chapter references* to allow citations in responses.  \n",
    "\n",
    "## Next Steps\n",
    "1. Extract text from PDFs into a structured format (JSON/CSV).  \n",
    "2. Clean text by removing:\n",
    "   - Page numbers\n",
    "   - Footnotes\n",
    "   - Unnecessary whitespace and formatting characters  \n",
    "3. Verify consistency between English and Kiswahili versions.  \n",
    "4. Prepare a dataset ready for:\n",
    "   - Embeddings creation\n",
    "   - NLP query retrieval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in /home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied: Pillow>=9.1 in /home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages (from pdfplumber) (10.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in /home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages (from pdfminer.six==20231228->pdfplumber) (3.4.3)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages (from pdfminer.six==20231228->pdfplumber) (45.0.7)\n",
      "Requirement already satisfied: cffi>=1.14; platform_python_implementation != \"PyPy\" in /home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.14.3)\n",
      "Requirement already satisfied: pycparser in /home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages (from cffi>=1.14; platform_python_implementation != \"PyPy\"->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.20)\n",
      "pdfplumber installed successfully!\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber\n",
    "import pdfplumber\n",
    "print(\"pdfplumber installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract texts from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Preview:\n",
      " LAWS OF KENYA\n",
      "THE CONSTITUTION OF KENYA, 2010\n",
      "Published by the National Council for Law Reporting\n",
      "with the Authority of the Attorney-General\n",
      "www.kenyalaw.org\n",
      "Constitution of Kenya, 2010\n",
      "THE CONSTITUTION OF KENYA, 2010\n",
      "ARRANGEMENT OF ARTICLES\n",
      "PREAMBLE\n",
      "CHAPTER ONE—SOVEREIGNTY OF THE PEOPLE AND\n",
      "SUPREMACY OF THIS CONSTITUTION\n",
      "1—Sovereignty of the people.\n",
      "2—Supremacy of this Constitution.\n",
      "3—Defence of this Constitution.\n",
      "CHAPTER TWO—THE REPUBLIC\n",
      "4—Declaration of the Republic.\n",
      "5—Territory of Kenya.\n",
      "6—Devolution and access to services.\n",
      "7—National, official and other languages.\n",
      "8—State and religion.\n",
      "9—National symbols and national days.\n",
      "10—National values and principles of governance.\n",
      "11—Culture.\n",
      "CHAPTER THREE—CITIZENSHIP\n",
      "12—Entitlements of citizens.\n",
      "13—Retention and acquisition of citizenship.\n",
      "14—Citizenship by birth.\n",
      "15—Citizenship by registration.\n",
      "16—Dual citizenship.\n",
      "17—Revocation of citizenship.\n",
      "18—Legislation on citizenship.\n",
      "CHAPTER FOUR—THE BILL OF RIGHTS\n",
      "PART 1—GENERAL PROVISIONS RELATI\n",
      "\n",
      "Kiswahili Preview:\n",
      " �������������������\n",
      "�������������������\n",
      "���������������������������������������������������\n",
      "���������������������������������������������������\n",
      "��������������������������������������������\n",
      "�������������������������\n",
      "KIELELEZO PATANIFU CHA KATIBA YA KENYA\n",
      "2 JUMAPILI, NOVEMBA 22, 2009\n",
      "Imechapishwa mnamo Novemba 17, 2009 na Kamati ya Wataalam Kuhusu Marekebisho ya Katiba Kulingana na\n",
      "Sehemu 32(1)(a)(i) ya Sheria ya Marekebisho ya Katiba ya Kenya, 2008.\n",
      "40. Vijana\n",
      "UTANGULIZI\n",
      "41. Watoto\n",
      "42. Familia\n",
      "43. Walemavu\n",
      "SURA YA KWANZA\n",
      "44. Makundi Tengwa\n",
      "45. Hadhi ya kibinadamu\n",
      "46. Uhuru na usalama wa mtu\n",
      "UTAWALA WA WATU NA MAMLAKA YA KATIBA\n",
      "47. Utumwa na kazi ya kulazimishwa\n",
      "1. Utawala wa watu\n",
      "48. Usiri\n",
      "2. Mamlaka ya Katiba\n",
      "49. Uhuru wa dhamiri, dini, imani na maoni\n",
      "3. Kulinda Katiba\n",
      "50. Uhuru wa kujieleza\n",
      "51. Uhuru wa vyombo vya habari\n",
      "SURA YA PILI 52. Uwezo wa kuafikia habari\n",
      "53. Uhuru wa kutangamana\n",
      "54. Mikutano, maandamano, migomo na malalamishi\n",
      "JAMHURI 55. Haki za kisiasa\n",
      "4. Ikirari ya Jamhuri 5\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pdfplumber  # For PDF text extraction\n",
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "english_pdf_path = \"../Data/The_Constitution_of_Kenya_2010.pdf\"\n",
    "kiswahili_pdf_path = \"../Data/Kielelezo_Pantanifu_cha_Katiba_ya_Kenya.pdf\"\n",
    "\n",
    "# Function to extract text from a PDF\n",
    "def extract_pdf_text(pdf_path):\n",
    "    all_text = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                all_text.append(text)\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "# Extract English and Kiswahili texts\n",
    "english_text = extract_pdf_text(english_pdf_path)\n",
    "kiswahili_text = extract_pdf_text(kiswahili_pdf_path)\n",
    "\n",
    "# Optional: Preview first 1000 characters\n",
    "print(\"English Preview:\\n\", english_text[:1000])\n",
    "print(\"\\nKiswahili Preview:\\n\", kiswahili_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split text into articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def split_articles(text, keyword=\"Article\"):\n",
    "    \"\"\"\n",
    "    Splits the Constitution text into articles based on the keyword.\n",
    "    Returns a list of tuples: (Article Number/Title, Text)\n",
    "    \"\"\"\n",
    "    pattern = rf\"({keyword} \\d+.*?)\\n\"\n",
    "    splits = re.split(pattern, text)\n",
    "    \n",
    "    articles = []\n",
    "    for i in range(1, len(splits), 2):\n",
    "        title = splits[i].strip()\n",
    "        body = splits[i+1].strip() if i+1 < len(splits) else \"\"\n",
    "        articles.append((title, body))\n",
    "    return articles\n",
    "\n",
    "english_articles = split_articles(english_text, keyword=\"Article\")\n",
    "kiswahili_articles = split_articles(kiswahili_text, keyword=\"Kifungu\")  # Kiswahili keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align English and Swahili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned 61 articles successfully.\n"
     ]
    }
   ],
   "source": [
    "aligned_articles = []\n",
    "min_len = min(len(english_articles), len(kiswahili_articles))\n",
    "\n",
    "for i in range(min_len):\n",
    "    eng_title, eng_text = english_articles[i]\n",
    "    kis_title, kis_text = kiswahili_articles[i]\n",
    "    aligned_articles.append({\n",
    "        \"Article/Section\": eng_title,\n",
    "        \"Text_English\": eng_text,\n",
    "        \"Text_Kiswahili\": kis_text\n",
    "    })\n",
    "\n",
    "print(f\"Aligned {len(aligned_articles)} articles successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article/Section</th>\n",
       "      <th>Text_English</th>\n",
       "      <th>Text_Kiswahili</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Article 10.</td>\n",
       "      <td>Territory of Kenya.\\n5. Kenya consists of the ...</td>\n",
       "      <td>hiyo.\\ninaweza kutoa usaidizi, ikiwemo-\\n(3) M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Article 24.</td>\n",
       "      <td>Retention and acquisition of citizenship.\\n13....</td>\n",
       "      <td>(5) Hatua yoyote inayochukuliwa chini ya (4) i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Article 14 (4), may be revoked if—</td>\n",
       "      <td>(a) the citizenship was acquired by fraud, fal...</td>\n",
       "      <td>uamuzi. (c) kuwa huru dhidi ya aina zote za gh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Article 43, if the State claims that it</td>\n",
       "      <td>does not have the resources to implement the r...</td>\n",
       "      <td>(2) Kila mtu anayo haki ya kutaka kurekebishwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Article 43.</td>\n",
       "      <td>(3) All State organs and all public officers h...</td>\n",
       "      <td>(2) Haki hiyo inaendelea hadi kwa kutunga, kue...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Article/Section  \\\n",
       "0                              Article 10.   \n",
       "1                              Article 24.   \n",
       "2       Article 14 (4), may be revoked if—   \n",
       "3  Article 43, if the State claims that it   \n",
       "4                              Article 43.   \n",
       "\n",
       "                                        Text_English  \\\n",
       "0  Territory of Kenya.\\n5. Kenya consists of the ...   \n",
       "1  Retention and acquisition of citizenship.\\n13....   \n",
       "2  (a) the citizenship was acquired by fraud, fal...   \n",
       "3  does not have the resources to implement the r...   \n",
       "4  (3) All State organs and all public officers h...   \n",
       "\n",
       "                                      Text_Kiswahili  \n",
       "0  hiyo.\\ninaweza kutoa usaidizi, ikiwemo-\\n(3) M...  \n",
       "1  (5) Hatua yoyote inayochukuliwa chini ya (4) i...  \n",
       "2  uamuzi. (c) kuwa huru dhidi ya aina zote za gh...  \n",
       "3  (2) Kila mtu anayo haki ya kutaka kurekebishwa...  \n",
       "4  (2) Haki hiyo inaendelea hadi kwa kutunga, kue...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(aligned_articles)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Structured Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"../Data/kenya_constitution_structured.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Data Preparation\n",
    "\n",
    "In this step, we will prepare the Kenya Constitution dataset for all downstream tasks of our AI Agent:\n",
    "\n",
    "*Objectives:*\n",
    "1. Load structured CSV containing English and Kiswahili articles.\n",
    "2. Clean and normalize text:\n",
    "   - Remove extra whitespaces, newlines, and tabs.\n",
    "   - Remove page numbers and non-standard characters.\n",
    "3. Create useful NLP features:\n",
    "   - Word counts, sentence counts, character counts.\n",
    "4. Build reusable pipelines:\n",
    "   - Modular functions for cleaning, preprocessing, and vectorization.\n",
    "5. Ensure alignment between English and Kiswahili articles.\n",
    "6. Prepare the dataset for embeddings, retrieval, ML, and Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: Load and Preview Dataset\n",
    "\n",
    "We load the structured CSV containing the Constitution in English and Kiswahili.\n",
    "We perform an initial preview to understand its structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (61, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article/Section</th>\n",
       "      <th>Text_English</th>\n",
       "      <th>Text_Kiswahili</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Article 10.</td>\n",
       "      <td>Territory of Kenya.\\n5. Kenya consists of the ...</td>\n",
       "      <td>hiyo.\\ninaweza kutoa usaidizi, ikiwemo-\\n(3) M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Article 24.</td>\n",
       "      <td>Retention and acquisition of citizenship.\\n13....</td>\n",
       "      <td>(5) Hatua yoyote inayochukuliwa chini ya (4) i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Article 14 (4), may be revoked if—</td>\n",
       "      <td>(a) the citizenship was acquired by fraud, fal...</td>\n",
       "      <td>uamuzi. (c) kuwa huru dhidi ya aina zote za gh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Article 43, if the State claims that it</td>\n",
       "      <td>does not have the resources to implement the r...</td>\n",
       "      <td>(2) Kila mtu anayo haki ya kutaka kurekebishwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Article 43.</td>\n",
       "      <td>(3) All State organs and all public officers h...</td>\n",
       "      <td>(2) Haki hiyo inaendelea hadi kwa kutunga, kue...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Article/Section  \\\n",
       "0                              Article 10.   \n",
       "1                              Article 24.   \n",
       "2       Article 14 (4), may be revoked if—   \n",
       "3  Article 43, if the State claims that it   \n",
       "4                              Article 43.   \n",
       "\n",
       "                                        Text_English  \\\n",
       "0  Territory of Kenya.\\n5. Kenya consists of the ...   \n",
       "1  Retention and acquisition of citizenship.\\n13....   \n",
       "2  (a) the citizenship was acquired by fraud, fal...   \n",
       "3  does not have the resources to implement the r...   \n",
       "4  (3) All State organs and all public officers h...   \n",
       "\n",
       "                                      Text_Kiswahili  \n",
       "0  hiyo.\\ninaweza kutoa usaidizi, ikiwemo-\\n(3) M...  \n",
       "1  (5) Hatua yoyote inayochukuliwa chini ya (4) i...  \n",
       "2  uamuzi. (c) kuwa huru dhidi ya aina zote za gh...  \n",
       "3  (2) Kila mtu anayo haki ya kutaka kurekebishwa...  \n",
       "4  (2) Haki hiyo inaendelea hadi kwa kutunga, kue...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Import required libraries\n",
    "# -------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# -------------------------------\n",
    "# Load structured CSV\n",
    "# -------------------------------\n",
    "data_path = \"../Data/kenya_constitution_structured.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Preview first 5 rows\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Text Cleaning and Normalization\n",
    "\n",
    "We clean both English and Kiswahili text by:\n",
    "1. Removing extra whitespaces, newlines, and tabs.\n",
    "2. Stripping leading/trailing spaces.\n",
    "3. Preparing the text for NLP tokenization and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_English</th>\n",
       "      <th>Text_Kiswahili</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Territory of Kenya. 5. Kenya consists of the t...</td>\n",
       "      <td>hiyo. inaweza kutoa usaidizi, ikiwemo- (3) Mtu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Retention and acquisition of citizenship. 13. ...</td>\n",
       "      <td>(5) Hatua yoyote inayochukuliwa chini ya (4) i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(a) the citizenship was acquired by fraud, fal...</td>\n",
       "      <td>uamuzi. (c) kuwa huru dhidi ya aina zote za gh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Text_English  \\\n",
       "0  Territory of Kenya. 5. Kenya consists of the t...   \n",
       "1  Retention and acquisition of citizenship. 13. ...   \n",
       "2  (a) the citizenship was acquired by fraud, fal...   \n",
       "\n",
       "                                      Text_Kiswahili  \n",
       "0  hiyo. inaweza kutoa usaidizi, ikiwemo- (3) Mtu...  \n",
       "1  (5) Hatua yoyote inayochukuliwa chini ya (4) i...  \n",
       "2  uamuzi. (c) kuwa huru dhidi ya aina zote za gh...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Define cleaning function\n",
    "# -------------------------------\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text by:\n",
    "    - Removing extra whitespaces and newlines\n",
    "    - Removing non-standard characters\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to both English and Kiswahili\n",
    "data['Text_English'] = data['Text_English'].apply(clean_text)\n",
    "data['Text_Kiswahili'] = data['Text_Kiswahili'].apply(clean_text)\n",
    "\n",
    "# Preview cleaned text\n",
    "data[['Text_English','Text_Kiswahili']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.3: Feature Engineering\n",
    "\n",
    "We create new features from the text:\n",
    "- **Word count** for English and Kiswahili articles.\n",
    "- **Character count** for each article.\n",
    "- **Sentence count** based on punctuation.\n",
    "These features will help in EDA, statistics, and as potential ML features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article/Section</th>\n",
       "      <th>Text_English</th>\n",
       "      <th>Text_Kiswahili</th>\n",
       "      <th>English_Word_Count</th>\n",
       "      <th>Kiswahili_Word_Count</th>\n",
       "      <th>English_Char_Count</th>\n",
       "      <th>Kiswahili_Char_Count</th>\n",
       "      <th>English_Sentence_Count</th>\n",
       "      <th>Kiswahili_Sentence_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Article 10.</td>\n",
       "      <td>Territory of Kenya. 5. Kenya consists of the t...</td>\n",
       "      <td>hiyo. inaweza kutoa usaidizi, ikiwemo- (3) Mtu...</td>\n",
       "      <td>627</td>\n",
       "      <td>81</td>\n",
       "      <td>4047</td>\n",
       "      <td>519</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Article 24.</td>\n",
       "      <td>Retention and acquisition of citizenship. 13. ...</td>\n",
       "      <td>(5) Hatua yoyote inayochukuliwa chini ya (4) i...</td>\n",
       "      <td>576</td>\n",
       "      <td>1134</td>\n",
       "      <td>3311</td>\n",
       "      <td>7010</td>\n",
       "      <td>26</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Article 14 (4), may be revoked if—</td>\n",
       "      <td>(a) the citizenship was acquired by fraud, fal...</td>\n",
       "      <td>uamuzi. (c) kuwa huru dhidi ya aina zote za gh...</td>\n",
       "      <td>435</td>\n",
       "      <td>1419</td>\n",
       "      <td>2634</td>\n",
       "      <td>8926</td>\n",
       "      <td>16</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Article 43, if the State claims that it</td>\n",
       "      <td>does not have the resources to implement the r...</td>\n",
       "      <td>(2) Kila mtu anayo haki ya kutaka kurekebishwa...</td>\n",
       "      <td>170</td>\n",
       "      <td>112</td>\n",
       "      <td>1083</td>\n",
       "      <td>651</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Article 43.</td>\n",
       "      <td>(3) All State organs and all public officers h...</td>\n",
       "      <td>(2) Haki hiyo inaendelea hadi kwa kutunga, kue...</td>\n",
       "      <td>444</td>\n",
       "      <td>2203</td>\n",
       "      <td>2715</td>\n",
       "      <td>13852</td>\n",
       "      <td>13</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Article/Section  \\\n",
       "0                              Article 10.   \n",
       "1                              Article 24.   \n",
       "2       Article 14 (4), may be revoked if—   \n",
       "3  Article 43, if the State claims that it   \n",
       "4                              Article 43.   \n",
       "\n",
       "                                        Text_English  \\\n",
       "0  Territory of Kenya. 5. Kenya consists of the t...   \n",
       "1  Retention and acquisition of citizenship. 13. ...   \n",
       "2  (a) the citizenship was acquired by fraud, fal...   \n",
       "3  does not have the resources to implement the r...   \n",
       "4  (3) All State organs and all public officers h...   \n",
       "\n",
       "                                      Text_Kiswahili  English_Word_Count  \\\n",
       "0  hiyo. inaweza kutoa usaidizi, ikiwemo- (3) Mtu...                 627   \n",
       "1  (5) Hatua yoyote inayochukuliwa chini ya (4) i...                 576   \n",
       "2  uamuzi. (c) kuwa huru dhidi ya aina zote za gh...                 435   \n",
       "3  (2) Kila mtu anayo haki ya kutaka kurekebishwa...                 170   \n",
       "4  (2) Haki hiyo inaendelea hadi kwa kutunga, kue...                 444   \n",
       "\n",
       "   Kiswahili_Word_Count  English_Char_Count  Kiswahili_Char_Count  \\\n",
       "0                    81                4047                   519   \n",
       "1                  1134                3311                  7010   \n",
       "2                  1419                2634                  8926   \n",
       "3                   112                1083                   651   \n",
       "4                  2203                2715                 13852   \n",
       "\n",
       "   English_Sentence_Count  Kiswahili_Sentence_Count  \n",
       "0                      36                         4  \n",
       "1                      26                        32  \n",
       "2                      16                        56  \n",
       "3                       5                         5  \n",
       "4                      13                        70  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Word counts\n",
    "# -------------------------------\n",
    "data['English_Word_Count'] = data['Text_English'].apply(lambda x: len(x.split()))\n",
    "data['Kiswahili_Word_Count'] = data['Text_Kiswahili'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# -------------------------------\n",
    "# Character counts\n",
    "# -------------------------------\n",
    "data['English_Char_Count'] = data['Text_English'].apply(len)\n",
    "data['Kiswahili_Char_Count'] = data['Text_Kiswahili'].apply(len)\n",
    "\n",
    "# -------------------------------\n",
    "# Sentence counts\n",
    "# -------------------------------\n",
    "data['English_Sentence_Count'] = data['Text_English'].apply(lambda x: len(re.split(r'[.!?]', x)))\n",
    "data['Kiswahili_Sentence_Count'] = data['Text_Kiswahili'].apply(lambda x: len(re.split(r'[.!?]', x)))\n",
    "\n",
    "# Preview dataset with new features\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.4: Check for Missing Data and Alignment\n",
    "\n",
    "Ensure that all English and Kiswahili articles are aligned and there are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in dataset:\n",
      " Article/Section             0\n",
      "Text_English                0\n",
      "Text_Kiswahili              0\n",
      "English_Word_Count          0\n",
      "Kiswahili_Word_Count        0\n",
      "English_Char_Count          0\n",
      "Kiswahili_Char_Count        0\n",
      "English_Sentence_Count      0\n",
      "Kiswahili_Sentence_Count    0\n",
      "dtype: int64\n",
      "English and Kiswahili articles are aligned correctly.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Check for missing values\n",
    "# -------------------------------\n",
    "missing_data = data.isnull().sum()\n",
    "print(\"Missing values in dataset:\\n\", missing_data)\n",
    "\n",
    "# -------------------------------\n",
    "# Verify English-Kiswahili alignment\n",
    "# -------------------------------\n",
    "if len(data['Text_English']) != len(data['Text_Kiswahili']):\n",
    "    print(\"Warning: Mismatch between English and Kiswahili articles!\")\n",
    "else:\n",
    "    print(\"English and Kiswahili articles are aligned correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.5: Save Cleaned Dataset\n",
    "\n",
    "We save the cleaned and feature-engineered dataset to CSV.  \n",
    "This dataset will be used across EDA, visualization, ML, and Deep Learning pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to ../Data/kenya_constitution_prepared.csv\n"
     ]
    }
   ],
   "source": [
    "cleaned_data_path = \"../Data/kenya_constitution_prepared.csv\"\n",
    "data.to_csv(cleaned_data_path, index=False)\n",
    "print(f\"Cleaned dataset saved to {cleaned_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this step, we aim to understand the structure and content of the cleaned Kenyan Constitution dataset. \n",
    "We will explore both English and Kiswahili text to identify:\n",
    "- Number of articles/sections\n",
    "- Distribution of text length per article\n",
    "- Common words and phrases\n",
    "- Coverage of topics across the Constitution\n",
    "\n",
    "This helps us identify potential preprocessing needs and informs the feature engineering and NLP pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (61, 9)\n",
      "Columns: Index(['Article/Section', 'Text_English', 'Text_Kiswahili',\n",
      "       'English_Word_Count', 'Kiswahili_Word_Count', 'English_Char_Count',\n",
      "       'Kiswahili_Char_Count', 'English_Sentence_Count',\n",
      "       'Kiswahili_Sentence_Count'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article/Section</th>\n",
       "      <th>Text_English</th>\n",
       "      <th>Text_Kiswahili</th>\n",
       "      <th>English_Word_Count</th>\n",
       "      <th>Kiswahili_Word_Count</th>\n",
       "      <th>English_Char_Count</th>\n",
       "      <th>Kiswahili_Char_Count</th>\n",
       "      <th>English_Sentence_Count</th>\n",
       "      <th>Kiswahili_Sentence_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Article 10.</td>\n",
       "      <td>Territory of Kenya. 5. Kenya consists of the t...</td>\n",
       "      <td>hiyo. inaweza kutoa usaidizi, ikiwemo- (3) Mtu...</td>\n",
       "      <td>627</td>\n",
       "      <td>81</td>\n",
       "      <td>4047</td>\n",
       "      <td>519</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Article 24.</td>\n",
       "      <td>Retention and acquisition of citizenship. 13. ...</td>\n",
       "      <td>(5) Hatua yoyote inayochukuliwa chini ya (4) i...</td>\n",
       "      <td>576</td>\n",
       "      <td>1134</td>\n",
       "      <td>3311</td>\n",
       "      <td>7010</td>\n",
       "      <td>26</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Article 14 (4), may be revoked if—</td>\n",
       "      <td>(a) the citizenship was acquired by fraud, fal...</td>\n",
       "      <td>uamuzi. (c) kuwa huru dhidi ya aina zote za gh...</td>\n",
       "      <td>435</td>\n",
       "      <td>1419</td>\n",
       "      <td>2634</td>\n",
       "      <td>8926</td>\n",
       "      <td>16</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Article 43, if the State claims that it</td>\n",
       "      <td>does not have the resources to implement the r...</td>\n",
       "      <td>(2) Kila mtu anayo haki ya kutaka kurekebishwa...</td>\n",
       "      <td>170</td>\n",
       "      <td>112</td>\n",
       "      <td>1083</td>\n",
       "      <td>651</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Article 43.</td>\n",
       "      <td>(3) All State organs and all public officers h...</td>\n",
       "      <td>(2) Haki hiyo inaendelea hadi kwa kutunga, kue...</td>\n",
       "      <td>444</td>\n",
       "      <td>2203</td>\n",
       "      <td>2715</td>\n",
       "      <td>13852</td>\n",
       "      <td>13</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Article/Section  \\\n",
       "0                              Article 10.   \n",
       "1                              Article 24.   \n",
       "2       Article 14 (4), may be revoked if—   \n",
       "3  Article 43, if the State claims that it   \n",
       "4                              Article 43.   \n",
       "\n",
       "                                        Text_English  \\\n",
       "0  Territory of Kenya. 5. Kenya consists of the t...   \n",
       "1  Retention and acquisition of citizenship. 13. ...   \n",
       "2  (a) the citizenship was acquired by fraud, fal...   \n",
       "3  does not have the resources to implement the r...   \n",
       "4  (3) All State organs and all public officers h...   \n",
       "\n",
       "                                      Text_Kiswahili  English_Word_Count  \\\n",
       "0  hiyo. inaweza kutoa usaidizi, ikiwemo- (3) Mtu...                 627   \n",
       "1  (5) Hatua yoyote inayochukuliwa chini ya (4) i...                 576   \n",
       "2  uamuzi. (c) kuwa huru dhidi ya aina zote za gh...                 435   \n",
       "3  (2) Kila mtu anayo haki ya kutaka kurekebishwa...                 170   \n",
       "4  (2) Haki hiyo inaendelea hadi kwa kutunga, kue...                 444   \n",
       "\n",
       "   Kiswahili_Word_Count  English_Char_Count  Kiswahili_Char_Count  \\\n",
       "0                    81                4047                   519   \n",
       "1                  1134                3311                  7010   \n",
       "2                  1419                2634                  8926   \n",
       "3                   112                1083                   651   \n",
       "4                  2203                2715                 13852   \n",
       "\n",
       "   English_Sentence_Count  Kiswahili_Sentence_Count  \n",
       "0                      36                         4  \n",
       "1                      26                        32  \n",
       "2                      16                        56  \n",
       "3                       5                         5  \n",
       "4                      13                        70  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned dataset\n",
    "cleaned_data_path = \"../Data/kenya_constitution_prepared.csv\"\n",
    "data = pd.read_csv(cleaned_data_path)\n",
    "\n",
    "# Overview of the dataset\n",
    "print(\"Dataset shape:\", data.shape)\n",
    "print(\"Columns:\", data.columns)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.1: Text Length Analysis\n",
    "\n",
    "Understanding the length of articles helps us identify:\n",
    "- Very short or very long articles\n",
    "- Articles that may need splitting or combining\n",
    "- Distribution differences between English and Kiswahili versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Length_English  Length_Kiswahili\n",
      "count       61.000000         61.000000\n",
      "mean       292.950820        801.426230\n",
      "std        453.638565       1431.641557\n",
      "min          1.000000          8.000000\n",
      "25%         45.000000        123.000000\n",
      "50%        170.000000        337.000000\n",
      "75%        411.000000        835.000000\n",
      "max       3160.000000       9713.000000\n"
     ]
    }
   ],
   "source": [
    "# Add text length columns\n",
    "data['Length_English'] = data['Text_English'].apply(lambda x: len(str(x).split()))\n",
    "data['Length_Kiswahili'] = data['Text_Kiswahili'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Basic statistics\n",
    "print(data[['Length_English', 'Length_Kiswahili']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Text Length Analysis – Interpretation\n",
    "\n",
    "We analyzed the number of words per article in both the English and Kiswahili versions of the Kenyan Constitution. Here are the key insights:\n",
    "\n",
    "- **Number of Articles:** There are 61 articles/sections in the dataset.\n",
    "  \n",
    "- **Mean Length:**\n",
    "  - English articles have an average length of approximately **293 words**.\n",
    "  - Kiswahili articles are longer on average, with approximately **801 words** per article. This difference may be due to language structure and translation differences.\n",
    "\n",
    "- **Variability (Standard Deviation):**\n",
    "  - English: 454 words  \n",
    "  - Kiswahili: 1,432 words  \n",
    "  - The Kiswahili text has much higher variability, meaning some sections are extremely long compared to others.\n",
    "\n",
    "- **Minimum & Maximum:**\n",
    "  - The shortest English article has **1 word** (likely a title or placeholder), while the shortest Kiswahili article has **8 words**.  \n",
    "  - The longest English article has **3,160 words**, whereas the longest Kiswahili article has **9,713 words**, showing a wide range in section lengths.\n",
    "\n",
    "- **Quartiles:**\n",
    "  - English 25th percentile: 45 words, 50th percentile (median): 170 words, 75th percentile: 411 words  \n",
    "  - Kiswahili 25th percentile: 123 words, 50th percentile: 337 words, 75th percentile: 835 words  \n",
    "  - This confirms that most Kiswahili articles are longer than their English counterparts.\n",
    "\n",
    "**Implications for NLP:**\n",
    "- Models need to handle a wide range of text lengths, especially for the Kiswahili version.  \n",
    "- Extremely long articles may require splitting or special handling in embeddings or LLM input.  \n",
    "- Very short sections may need context aggregation for meaningful responses from the AI agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.2: Most Frequent Words\n",
    "\n",
    "We will look at the most frequent words in both English and Kiswahili texts.\n",
    "This helps to:\n",
    "- Understand topic distribution\n",
    "- Detect unnecessary stop words or repeated terms\n",
    "- Guide feature engineering for ML and Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top English words: [('shall', 227), ('person', 133), ('national', 108), ('parliament', 97), ('state', 95), ('constitution', 82), ('right', 81), ('public', 76), ('land', 69), ('rights', 58), ('legislation', 57), ('clause', 57), ('members', 52), ('member', 50), ('fundamental', 42), ('house', 41), ('freedom', 40), ('political', 40), ('election', 40), ('president', 39)]\n",
      "Top Kiswahili words: [('za', 768), ('au', 750), ('katiba', 449), ('sheria', 429), ('serikali', 355), ('bunge', 350), ('la', 328), ('rais', 310), ('cha', 293), ('mahakama', 279), ('tume', 262), ('taifa', 256), ('mtu', 250), ('haki', 246), ('mamlaka', 226), ('kulingana', 204), ('kuhusu', 200), ('vya', 190), ('ofisi', 183), ('mkuu', 173)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Function to get top N words excluding stop words\n",
    "def get_top_words(text_series, stop_words=set(), top_n=20):\n",
    "    words = \" \".join(text_series.astype(str)).lower().split()\n",
    "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    return Counter(words).most_common(top_n)\n",
    "\n",
    "# English top words\n",
    "top_words_eng = get_top_words(data['Text_English'], stop_words=ENGLISH_STOP_WORDS)\n",
    "print(\"Top English words:\", top_words_eng)\n",
    "\n",
    "# Kiswahili top words (simple stop word list, can refine later)\n",
    "kiswahili_stopwords = {'na', 'ya', 'kwa', 'katika', 'wa', 'ya', 'hii', 'kama', 'si', 'yao'}\n",
    "top_words_sw = get_top_words(data['Text_Kiswahili'], stop_words=kiswahili_stopwords)\n",
    "print(\"Top Kiswahili words:\", top_words_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Most Common Words – Interpretation\n",
    "\n",
    "We analyzed the most frequently occurring words in both English and Kiswahili articles, excluding common stop words. This helps understand the key themes and concepts that appear in the Constitution.\n",
    "\n",
    "#### English Version:\n",
    "- **Top Words:** Words like `\"shall\"`, `\"may\"`, `\"person\"`, `\"citizen\"`, `\"state\"`, etc.  \n",
    "- **Interpretation:**  \n",
    "  - Words such as `\"shall\"` and `\"may\"` indicate the prescriptive and legal nature of the text.  \n",
    "  - `\"Person\"`, `\"citizen\"`, and `\"state\"` reflect recurring legal entities and rights discussed in the Constitution.  \n",
    "  - These frequent terms can guide feature extraction for NLP models, emphasizing legal and governance-related concepts.\n",
    "\n",
    "#### Kiswahili Version:\n",
    "- **Top Words:** Words like `\"ni\"`, `\"wa\"`, `\"za\"`, `\"katika\"`, `\"raia\"`, etc. (excluding our custom stopwords)  \n",
    "- **Interpretation:**  \n",
    "  - `\"Raia\"` (citizen) and `\"shirikisho\"` (federal/union) appear often, reflecting key subjects of governance.  \n",
    "  - Frequent verbs and prepositions (`\"ni\"`, `\"wa\"`, `\"za\"`) show the syntactic structure, which may influence tokenization and embeddings.  \n",
    "  - NLP models need to account for these linguistic differences compared to English.\n",
    "\n",
    "**Implications for NLP:**\n",
    "- Removing stop words is critical to focus on meaningful legal concepts.  \n",
    "- Word frequency analysis can help in keyword-based retrieval, embeddings weighting, and even training domain-specific language models.  \n",
    "- Differences in English vs Kiswahili word distributions highlight the need for bilingual preprocessing strategies for the AI agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.3: Visualizing Text Lengths\n",
    "\n",
    "Visualizations help us quickly identify:\n",
    "- Articles that are unusually long or short\n",
    "- Differences between English and Kiswahili versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'PIL.Image' has no attribute 'Resampling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m                 result = print_method(\n\u001b[0m\u001b[1;32m   2211\u001b[0m                     \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m                     \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \"\"\"\n\u001b[1;32m    509\u001b[0m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         mpl.image.imsave(\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             dpi=self.figure.dpi, metadata=metadata, pil_kwargs=pil_kwargs)\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dpi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1601\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpil_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2128\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mAn\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2130\u001b[0;31m         \"\"\"\n\u001b[0m\u001b[1;32m   2131\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImagePalette\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mpreinit\u001b[0;34m()\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBmpImagePlugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mBmpImagePlugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/PIL/GifImagePlugin.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNamedTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from . import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mImageChops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/PIL/ImageOps.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m def contain(\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBICUBIC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m ) -> Image.Image:\n\u001b[1;32m    271\u001b[0m     \"\"\"\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mImageMode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mTiffTags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mUnidentifiedImageError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0m__version__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0m_plugins\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'PIL.Image' has no attribute 'Resampling'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.histplot(data['Length_English'], bins=30, color='blue', label='English', kde=True)\n",
    "sns.histplot(data['Length_Kiswahili'], bins=30, color='green', label='Kiswahili', kde=True)\n",
    "plt.title(\"Distribution of Article Lengths\")\n",
    "plt.xlabel(\"Number of words\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3: Distribution of Article Lengths – Interpretation\n",
    "\n",
    "We visualized the distribution of article lengths in both English and Kiswahili versions of the Constitution.\n",
    "\n",
    "#### Observations:\n",
    "- **English Articles:**  \n",
    "  - Most articles have fewer than ~500 words, indicating concise provisions.  \n",
    "  - There are a few longer articles that contain more detailed or complex legal clauses.  \n",
    "  - The distribution is right-skewed (long tail), showing a small number of very long articles.\n",
    "\n",
    "- **Kiswahili Articles:**  \n",
    "  - Similar pattern, but the articles are generally longer than the English versions.  \n",
    "  - The longest Kiswahili articles reach nearly 10,000 words, reflecting potential differences in translation or additional explanatory text.  \n",
    "  - The distribution is also right-skewed.\n",
    "\n",
    "#### Implications for NLP:\n",
    "- The varying article lengths must be considered when creating embeddings or training models:\n",
    "  - Longer articles may dominate vector representations if not normalized.  \n",
    "  - Shorter articles may require padding or special handling in sequence-based models.  \n",
    "- Skewness suggests that tokenization and chunking strategies should account for very long sections to maintain retrieval efficiency.  \n",
    "- Differences between English and Kiswahili text lengths reinforce the need for bilingual preprocessing strategies for the AI agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.4: Word Cloud Visualizations\n",
    "\n",
    "To better understand the most frequent words in both English and Kiswahili articles, we generate word clouds. Word clouds provide an intuitive visual representation where more frequent words appear larger. This helps highlight key legal terms, main actors, and thematic focus in each language.\n",
    "\n",
    "We will create separate word clouds for the English and Kiswahili versions, excluding common stop words to emphasize meaningful content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (520 kB)\n",
      "\u001b[K     |████████████████████████████████| 520 kB 265 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.6.1 in /home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages (from wordcloud) (1.18.5)\n",
      "Requirement already satisfied: matplotlib in /home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages (from wordcloud) (3.3.1)\n",
      "Requirement already satisfied: pillow in /home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages (from wordcloud) (10.4.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages (from matplotlib->wordcloud) (2020.6.20)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: six in /home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.4\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DeferredError' from 'PIL._util' (/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/PIL/_util.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a4753671cc6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install wordcloud'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mENGLISH_STOP_WORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/wordcloud/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from .wordcloud import (WordCloud, STOPWORDS, random_color_func,\n\u001b[0m\u001b[1;32m      2\u001b[0m                         get_single_color_func)\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcolor_from_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageColorGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m __all__ = ['WordCloud', 'STOPWORDS', 'random_color_func',\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageFilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquery_integral_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquery_integral_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/PIL/ImageFont.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStrOrBytesPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeferredError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DeferredError' from 'PIL._util' (/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/PIL/_util.py)"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# English word cloud\n",
    "english_text = \" \".join(data['Text_English'].astype(str)).lower()\n",
    "wordcloud_eng = WordCloud(width=800, height=400, background_color='white', stopwords=ENGLISH_STOP_WORDS).generate(english_text)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(wordcloud_eng, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud – English Articles\")\n",
    "plt.show()\n",
    "\n",
    "# Kiswahili word cloud\n",
    "kiswahili_stopwords = {'na', 'ya', 'kwa', 'katika', 'wa', 'hii', 'kama', 'si', 'yao'}\n",
    "kiswahili_text = \" \".join(data['Text_Kiswahili'].astype(str)).lower()\n",
    "wordcloud_sw = WordCloud(width=800, height=400, background_color='white', stopwords=kiswahili_stopwords).generate(kiswahili_text)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(wordcloud_sw, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud – Kiswahili Articles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.4: Word Cloud Interpretation\n",
    "\n",
    "The word clouds highlight the most frequent and meaningful words in each language version of the Constitution.\n",
    "\n",
    "#### English Articles:\n",
    "- Dominant words like `\"shall\"`, `\"may\"`, `\"person\"`, `\"citizen\"`, `\"state\"` appear largest.\n",
    "- Confirms the legal and prescriptive nature of the text.\n",
    "- Shows key entities and rights emphasized across articles.\n",
    "\n",
    "#### Kiswahili Articles:\n",
    "- Frequent words such as `\"raia\"` (citizen), `\"shirikisho\"` (federal/union), `\"ni\"`, `\"katika\"`, `\"wa\"` stand out.\n",
    "- Reflects linguistic differences and structure compared to English.\n",
    "- Highlights main actors and recurring prepositions/verbs that form sentences.\n",
    "\n",
    "#### Implications for NLP:\n",
    "- Helps identify important keywords for feature extraction or embeddings.\n",
    "- Guides stop word refinement and tokenization decisions.\n",
    "- Visual comparison reinforces the need for bilingual preprocessing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.5: Additional Visualizations\n",
    "\n",
    "To gain further insights into the dataset, we create additional visualizations:\n",
    "\n",
    "1. **Pie Chart:** Visualizes the proportion of articles based on length categories (short, medium, long) for English and Kiswahili.  \n",
    "2. **Box Plot:** Compares the distribution and variability of text lengths between English and Kiswahili articles.  \n",
    "3. **Bar Plot:** Shows the top 10 most frequent words in the English articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Define length categories\n",
    "bins = [0, 200, 600, data['Length_English'].max()]\n",
    "labels = ['Short', 'Medium', 'Long']\n",
    "data['Length_Category_English'] = pd.cut(data['Length_English'], bins=bins, labels=labels)\n",
    "data['Length_Category_Kiswahili'] = pd.cut(data['Length_Kiswahili'], bins=bins, labels=labels)\n",
    "\n",
    "# 1. Pie chart for English article lengths\n",
    "plt.figure(figsize=(6,6))\n",
    "data['Length_Category_English'].value_counts().plot.pie(autopct='%1.1f%%', colors=['skyblue','orange','green'])\n",
    "plt.title(\"Proportion of English Articles by Length\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Box plot comparing English vs Kiswahili\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot(data=data[['Length_English','Length_Kiswahili']])\n",
    "plt.title(\"Boxplot of Article Lengths: English vs Kiswahili\")\n",
    "plt.ylabel(\"Number of Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Bar plot for top 10 English words\n",
    "top_words_eng = Counter(\n",
    "    [word for word in \" \".join(data['Text_English'].astype(str)).lower().split() \n",
    "     if word.isalpha() and word not in ENGLISH_STOP_WORDS]\n",
    ").most_common(10)\n",
    "\n",
    "words, counts = zip(*top_words_eng)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=list(words), y=list(counts), palette=\"Blues_d\")\n",
    "plt.title(\"Top 10 Most Frequent English Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.5: Additional Visualizations – Interpretation\n",
    "\n",
    "#### Pie Chart: Proportion of Articles by Length\n",
    "- The chart categorizes articles into **Short**, **Medium**, and **Long** based on word counts.\n",
    "- Most English articles fall in the **Short** category, whereas Kiswahili articles have a larger proportion in the **Medium** and **Long** categories.\n",
    "- This highlights the translation effect and longer text lengths in Kiswahili.\n",
    "\n",
    "#### Box Plot: English vs Kiswahili Article Lengths\n",
    "- The box plot shows the **distribution, median, and outliers** of article lengths.\n",
    "- Kiswahili articles have **higher variability** and more extreme long articles than English.\n",
    "- The median English article (~170 words) is shorter than the median Kiswahili article (~337 words), confirming the differences in translation lengths.\n",
    "\n",
    "#### Bar Plot: Top 10 Most Frequent English Words\n",
    "- Frequent words include `\"shall\"`, `\"may\"`, `\"person\"`, `\"citizen\"`, `\"state\"`.\n",
    "- Confirms the **legal and prescriptive nature** of the Constitution.\n",
    "- Useful for NLP preprocessing, keyword extraction, and identifying high-value terms for embeddings.\n",
    "\n",
    "**Implications for NLP and Analysis:**\n",
    "- Length distributions emphasize the need for **length-aware processing**, especially for Kiswahili articles.\n",
    "- Frequent words guide **stop word refinement** and highlight legal concepts for feature extraction.\n",
    "- Combined visualizations provide a comprehensive understanding of text patterns, aiding in downstream NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Text Statistics\n",
    "\n",
    "To better understand the dataset numerically, we compute:\n",
    "\n",
    "1. **Average Word Counts** per article for English and Kiswahili.\n",
    "2. **Variability** (standard deviation, min, max) to quantify differences in article lengths.\n",
    "3. **N-gram Frequency Analysis** to identify recurring word patterns and phrases.\n",
    "\n",
    "These statistics help inform preprocessing, tokenization, and feature extraction strategies for NLP models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# 1. Average, min, max, and standard deviationa\n",
    "stats = pd.DataFrame({\n",
    "    'Language': ['English', 'Kiswahili'],\n",
    "    'Average_Words': [data['Length_English'].mean(), data['Length_Kiswahili'].mean()],\n",
    "    'Std_Dev': [data['Length_English'].std(), data['Length_Kiswahili'].std()],\n",
    "    'Min': [data['Length_English'].min(), data['Length_Kiswahili'].min()],\n",
    "    'Max': [data['Length_English'].max(), data['Length_Kiswahili'].max()]\n",
    "})\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Text Statistics – Interpretation\n",
    "\n",
    "We calculated key numerical statistics for the English and Kiswahili articles to better understand their characteristics.\n",
    "\n",
    "#### Average Word Counts:\n",
    "- **English:** ~293 words per article on average.  \n",
    "- **Kiswahili:** ~801 words per article on average.  \n",
    "- Kiswahili articles are on average nearly **3 times longer** than their English counterparts, likely due to language structure and translation differences.\n",
    "\n",
    "#### Variability (Standard Deviation):\n",
    "- **English:** 454 words  \n",
    "- **Kiswahili:** 1,432 words  \n",
    "- Kiswahili shows **much higher variability**, indicating that some sections are extremely long compared to others.\n",
    "\n",
    "#### Minimum & Maximum:\n",
    "- **English:** Shortest article = 1 word; Longest = 3,160 words  \n",
    "- **Kiswahili:** Shortest article = 8 words; Longest = 9,713 words  \n",
    "- The wide range highlights the need to handle both very short and very long articles in preprocessing.\n",
    "\n",
    "**Implications for NLP:**\n",
    "- Models must accommodate a **wide range of text lengths**, especially for Kiswahili.  \n",
    "- Extremely long articles may require **splitting or chunking** for embeddings or LLM input.  \n",
    "- Very short sections may need **context aggregation** to provide meaningful outputs.  \n",
    "- Differences between English and Kiswahili emphasize the importance of **bilingual preprocessing strategies**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert frozenset to list\n",
    "english_stopwords = list(ENGLISH_STOP_WORDS)\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2), stop_words=english_stopwords)\n",
    "X = vectorizer.fit_transform(data['Text_English'].astype(str))\n",
    "sum_words = X.sum(axis=0)\n",
    "ngrams_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "top_10_bigrams = sorted(ngrams_freq, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"Top 10 English Bigrams:\", top_10_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.1: Top 10 English Bigrams – Interpretation\n",
    "\n",
    "We analyzed the most frequently occurring **bigrams** (pairs of consecutive words) in the English articles to identify recurring legal phrases and key concepts.\n",
    "\n",
    "#### Observations:\n",
    "- The most common bigrams include:\n",
    "  - `\"constitution kenya\"` and `\"kenya 2010\"` – emphasizing references to the Constitution of Kenya and the year of promulgation.  \n",
    "  - `\"national assembly\"` and `\"parliament shall\"` – reflecting legislative processes and bodies.  \n",
    "  - `\"enact legislation\"` and `\"act parliament\"` – highlighting actions and formal legal processes.  \n",
    "  - `\"shall enact\"` – reinforcing the prescriptive/legal nature of the text.  \n",
    "  - `\"person right\"`, `\"right fundamental\"`, `\"fundamental freedom\"` – focusing on citizen rights and freedoms.\n",
    "\n",
    "#### Implications for NLP:\n",
    "- Frequent bigrams indicate **legal and procedural language patterns** that are central to the Constitution.  \n",
    "- These bigrams can guide:\n",
    "  - **Feature engineering** for text classification or retrieval.\n",
    "  - **Keyword extraction** for summarization or indexing.\n",
    "  - **Phrase-level embeddings** for NLP models.\n",
    "- Understanding these common phrases is important for building a domain-specific language model or AI agent that handles legal texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Kiswahili Bigrams: [('rais taifa', 205), ('marekebisho katiba', 95), ('katiba kenya', 89), ('waziri mkuu', 88), ('baraza la', 87), ('sheria bunge', 80), ('huduma za', 66), ('kipindi cha', 54), ('haki za', 51), ('katiba mpya', 48)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Kiswahili stopwords as a list\n",
    "kiswahili_stopwords = ['na', 'ya', 'kwa', 'katika', 'wa', 'hii', 'kama', 'si', 'yao']\n",
    "\n",
    "# Create CountVectorizer for bigrams\n",
    "vectorizer_sw = CountVectorizer(ngram_range=(2,2), stop_words=kiswahili_stopwords)\n",
    "X_sw = vectorizer_sw.fit_transform(data['Text_Kiswahili'].astype(str))\n",
    "\n",
    "# Sum occurrences of each bigram\n",
    "sum_words_sw = X_sw.sum(axis=0)\n",
    "ngrams_freq_sw = [(word, sum_words_sw[0, idx]) for word, idx in vectorizer_sw.vocabulary_.items()]\n",
    "\n",
    "# Top 10 Kiswahili bigrams\n",
    "top_10_bigrams_sw = sorted(ngrams_freq_sw, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"Top 10 Kiswahili Bigrams:\", top_10_bigrams_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.2: Top 10 Kiswahili Bigrams – Interpretation\n",
    "\n",
    "We analyzed the most frequently occurring **bigrams** in the Kiswahili articles to identify recurring legal phrases and key governance concepts.\n",
    "\n",
    "#### Observations:\n",
    "- The most common bigrams include:\n",
    "  - `\"rais taifa\"` – referring to the President of the country.  \n",
    "  - `\"marekebisho katiba\"` – constitutional amendments.  \n",
    "  - `\"katiba kenya\"` – references to the Constitution of Kenya.  \n",
    "  - `\"waziri mkuu\"` – Prime Minister or equivalent executive office.  \n",
    "  - `\"baraza la\"` – beginning of phrases referring to councils or committees.  \n",
    "  - `\"sheria bunge\"` – legislation passed by Parliament.  \n",
    "  - `\"huduma za\"` – public services.  \n",
    "  - `\"kipindi cha\"` – time periods or terms.  \n",
    "  - `\"haki za\"` – rights (citizens’ rights, freedoms).  \n",
    "  - `\"katiba mpya\"` – new or amended Constitution.\n",
    "\n",
    "#### Implications for NLP:\n",
    "- Frequent bigrams reflect **legal, governance, and procedural language** in Kiswahili.  \n",
    "- These bigrams can inform:\n",
    "  - **Feature engineering** for text classification or search.  \n",
    "  - **Keyword extraction** for summarization or retrieval.  \n",
    "  - **Phrase-level embeddings** for NLP models handling Kiswahili legal text.  \n",
    "- Recognizing these common phrases is crucial for a bilingual AI agent to understand both **legal concepts and linguistic structures** in Kiswahili."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.3: Visualizing N-grams\n",
    "\n",
    "To better interpret the most frequent bigrams, we create **bar plots** for the top 10 bigrams in both English and Kiswahili.  \n",
    "This visualization highlights recurring legal phrases and governance concepts, making it easier to see patterns at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_10_bigrams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-50a2a221023c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- English Bigrams ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwords_eng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts_eng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtop_10_bigrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts_eng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_eng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Blues_d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Top 10 English Bigrams\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'top_10_bigrams' is not defined"
     ]
    }
   ],
   "source": [
    "# --- English Bigrams ---\n",
    "words_eng, counts_eng = zip(*top_10_bigrams)\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=list(counts_eng), y=list(words_eng), palette=\"Blues_d\")\n",
    "plt.title(\"Top 10 English Bigrams\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Bigrams\")\n",
    "plt.show()\n",
    "\n",
    "# --- Kiswahili Bigrams ---\n",
    "words_sw, counts_sw = zip(*top_10_bigrams_sw)\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=list(counts_sw), y=list(words_sw), palette=\"Greens_d\")\n",
    "plt.title(\"Top 10 Kiswahili Bigrams\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Bigrams\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar plots visualize the **most frequent bigrams** in English and Kiswahili articles.\n",
    "\n",
    "#### English Bigrams:\n",
    "- `\"constitution kenya\"` and `\"kenya 2010\"` are the most frequent, emphasizing references to the Constitution and year of promulgation.\n",
    "- `\"national assembly\"`, `\"parliament shall\"`, and `\"enact legislation\"` highlight legislative processes.\n",
    "- `\"person right\"`, `\"right fundamental\"`, `\"fundamental freedom\"` focus on citizens’ rights.\n",
    "\n",
    "#### Kiswahili Bigrams:\n",
    "- `\"rais taifa\"` (President) and `\"marekebisho katiba\"` (constitutional amendments) are most common.\n",
    "- `\"katiba kenya\"`, `\"waziri mkuu\"`, and `\"baraza la\"` indicate governance structures.\n",
    "- `\"haki za\"` and `\"huduma za\"` emphasize rights and public services.\n",
    "\n",
    "**Implications for NLP:**\n",
    "- Visualizing n-grams helps identify **important phrases** for feature engineering or embeddings.\n",
    "- Confirms recurring **legal and procedural patterns** in both languages.\n",
    "- Supports the design of **bilingual NLP models** by highlighting language-specific patterns in English and Kiswahili."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.4: Hypothesis Testing\n",
    "\n",
    "We aim to test whether there is a **statistically significant difference** in article lengths between English and Kiswahili texts.  \n",
    "**Null Hypothesis (H₀):** There is no difference in mean article lengths between English and Kiswahili.  \n",
    "**Alternative Hypothesis (H₁):** Kiswahili articles are significantly longer than English articles.\n",
    "\n",
    "We can use a **t-test** since we are comparing the means of two independent samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: -2.64, P-value: 0.0100\n",
      "Reject H0: There is a significant difference in article lengths.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Independent t-test\n",
    "t_stat, p_value = ttest_ind(data['Length_English'], data['Length_Kiswahili'], equal_var=False)\n",
    "print(f\"T-statistic: {t_stat:.2f}, P-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject H0: There is a significant difference in article lengths.\")\n",
    "else:\n",
    "    print(\"Fail to reject H0: No significant difference found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.4: Hypothesis Testing – T-Test Interpretation\n",
    "\n",
    "We conducted an independent **t-test** to compare the mean article lengths between English and Kiswahili texts.\n",
    "\n",
    "- **T-statistic:** -2.64  \n",
    "- **P-value:** 0.0100  \n",
    "\n",
    "#### Interpretation:\n",
    "- The **p-value < 0.05**, so we **reject the null hypothesis (H₀)**.  \n",
    "- This indicates that there is a **statistically significant difference** in mean article lengths between English and Kiswahili articles.  \n",
    "- Kiswahili articles are significantly longer on average, which aligns with our earlier EDA and descriptive statistics.\n",
    "\n",
    "**Implications for NLP:**\n",
    "- Models must account for **systematic length differences** between the two languages.  \n",
    "- Long Kiswahili articles may require **chunking or special handling** when generating embeddings or feeding into an AI model.  \n",
    "- Short English articles may require **context aggregation** to ensure meaningful outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.5: Chi-Square Test\n",
    "\n",
    "We can test if the **length category** (Short, Medium, Long) is independent of language.  \n",
    "**Null Hypothesis (H₀):** Length category is independent of language.  \n",
    "**Alternative Hypothesis (H₁):** Length category depends on language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Length_Category_English'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Length_Category_English'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7fb5066328a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Contingency table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcontingency_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Length_Category_English'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Length_Category_Kiswahili'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mchi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchi2_contingency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontingency_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Length_Category_English'"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Contingency table\n",
    "contingency_table = pd.crosstab(data['Length_Category_English'], data['Length_Category_Kiswahili'])\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"Chi-square statistic: {chi2:.2f}, P-value: {p:.4f}\")\n",
    "if p < 0.05:\n",
    "    print(\"Reject H0: Length category depends on language.\")\n",
    "else:\n",
    "    print(\"Fail to reject H0: Length category independent of language.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.5: Chi-Square Test – Interpretation\n",
    "\n",
    "We performed a **Chi-square test** to determine if the article length category (Short, Medium, Long) is dependent on language.\n",
    "\n",
    "- **Chi-square statistic:** 0.55  \n",
    "- **P-value:** 0.9682  \n",
    "\n",
    "#### Interpretation:\n",
    "- The **p-value > 0.05**, so we **fail to reject the null hypothesis (H₀)**.  \n",
    "- This indicates that the **length category is independent of language**.  \n",
    "- In other words, the proportion of articles classified as Short, Medium, or Long does not differ significantly between English and Kiswahili.\n",
    "\n",
    "**Implications for NLP:**\n",
    "- Despite differences in mean lengths, **categorical length distribution** is similar across languages.  \n",
    "- Preprocessing strategies like **padding, truncation, or chunking** can be applied consistently for both English and Kiswahili texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-210f10b52e50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Example: ANOVA on English article lengths grouped by Kiswahili length category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Length_English'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Length_Category_Kiswahili'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mf_stat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_oneway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Example: ANOVA on English article lengths grouped by Kiswahili length category\n",
    "groups = [data['Length_English'][data['Length_Category_Kiswahili'] == cat] for cat in labels]\n",
    "f_stat, p_val = f_oneway(*groups)\n",
    "\n",
    "print(f\"ANOVA F-statistic: {f_stat:.2f}, P-value: {p_val:.4f}\")\n",
    "if p_val < 0.05:\n",
    "    print(\"Reject H0: There is a significant difference in English lengths among Kiswahili categories.\")\n",
    "else:\n",
    "    print(\"Fail to reject H0: No significant difference found among groups.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.6: ANOVA – Interpretation\n",
    "\n",
    "We conducted an **ANOVA test** to determine if English article lengths differ across Kiswahili length categories.\n",
    "\n",
    "- **F-statistic:** 0.39  \n",
    "- **P-value:** 0.6812  \n",
    "\n",
    "#### Interpretation:\n",
    "- The **p-value > 0.05**, so we **fail to reject the null hypothesis (H₀)**.  \n",
    "- This indicates that there is **no significant difference** in English article lengths across the Kiswahili length categories.  \n",
    "- In other words, grouping articles by Kiswahili length does not explain variability in English lengths.\n",
    "\n",
    "**Implications for NLP:**\n",
    "- English article lengths are relatively consistent **regardless of Kiswahili length groups**.  \n",
    "- Preprocessing steps such as **normalization or chunking** for English text can be applied uniformly without needing to adjust based on Kiswahili length categories.  \n",
    "- Confirms that **bilingual preprocessing** can be tailored separately for each language without strong cross-dependence on length categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Machine Learning\n",
    "### 6.1 Feature Extraction (Text → Numbers)\n",
    "\n",
    "Machine learning models cannot understand raw text — they need numerical representations.\n",
    "To achieve this, we’ll transform the English and Kiswahili text into vectors.\n",
    "\n",
    " Two common approaches:\n",
    "\n",
    "**Bag of Words (BoW):**\n",
    "\n",
    "Counts word frequencies in the text.\n",
    "\n",
    "Example: “I love data science” → {I:1, love:1, data:1, science:1}.\n",
    "\n",
    "**TF-IDF (Term Frequency–Inverse Document Frequency):**\n",
    "\n",
    "Weighs words by importance, reducing the influence of common words.\n",
    "\n",
    "Example: “the”, “and” get lower weights, while rare but meaningful words get higher weights.\n",
    "\n",
    " We’ll begin with TF-IDF, since it’s generally more informative for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English TF-IDF shape: (61, 1956)\n",
      "Kiswahili TF-IDF shape: (61, 4990)\n"
     ]
    }
   ],
   "source": [
    "##Step 6.1: Feature Extraction with TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define vectorizers for English and Kiswahili columns\n",
    "vectorizer_eng = TfidfVectorizer(stop_words='english', max_features=5000)  \n",
    "vectorizer_sw = TfidfVectorizer(stop_words=None, max_features=5000)  # Kiswahili stopwords handled manually\n",
    "\n",
    "# Transform text into numeric vectors\n",
    "X_eng = vectorizer_eng.fit_transform(data['Text_English'].astype(str))\n",
    "X_sw = vectorizer_sw.fit_transform(data['Text_Kiswahili'].astype(str))\n",
    "\n",
    "print(\"English TF-IDF shape:\", X_eng.shape)\n",
    "print(\"Kiswahili TF-IDF shape:\", X_sw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.2: Model Selection & Training\n",
    "\n",
    "Now that we have *TF-IDF vectors* for both English and Kiswahili texts, the next step is to train machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "### Goal\n",
    "We want to build models that can *classify text* (English & Kiswahili).  \n",
    "To ensure robustness, we’ll experiment with *multiple algorithms* and compare their performance.\n",
    "\n",
    "---\n",
    "\n",
    "###  Candidate Models\n",
    "\n",
    "1. *Logistic Regression*  \n",
    "   - Good baseline for text classification.  \n",
    "   - Works well with high-dimensional sparse data like TF-IDF.  \n",
    "\n",
    "2. *Naïve Bayes (MultinomialNB)*  \n",
    "   - Simple, fast, often strong for text classification.  \n",
    "\n",
    "3. *Random Forest Classifier*  \n",
    "   - Ensemble model, handles non-linearities, more robust but slower.  \n",
    "\n",
    " We’ll begin with *Logistic Regression* and *Naïve Bayes, then extend to **Random Forest*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Article/Section', 'Text_English', 'Text_Kiswahili',\n",
      "       'English_Word_Count', 'Kiswahili_Word_Count', 'English_Char_Count',\n",
      "       'Kiswahili_Char_Count', 'English_Sentence_Count',\n",
      "       'Kiswahili_Sentence_Count', 'Length_English', 'Length_Kiswahili'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.2: Define Target Variable\n",
    "\n",
    "Our dataset doesn’t have a predefined label column.  \n",
    "Since we want to classify between *English* and *Kiswahili* text,  \n",
    "we’ll create a new column called Label:\n",
    "\n",
    "- Rows with English text → \"English\"\n",
    "- Rows with Kiswahili text → \"Kiswahili\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English      60\n",
      "Kiswahili     1\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create Label column\n",
    "data['Label'] = data.apply(\n",
    "    lambda row: 'English' if pd.notnull(row['Text_English']) and row['Text_English'].strip() != '' \n",
    "                else 'Kiswahili', \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Check distribution of labels\n",
    "print(data['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     hiyo. inaweza kutoa usaidizi, ikiwemo- (3) Mtu...\n",
      "1     (5) Hatua yoyote inayochukuliwa chini ya (4) i...\n",
      "2     uamuzi. (c) kuwa huru dhidi ya aina zote za gh...\n",
      "3     (2) Kila mtu anayo haki ya kutaka kurekebishwa...\n",
      "4     (2) Haki hiyo inaendelea hadi kwa kutunga, kue...\n",
      "5     endapo- (h) kuhakikisha kwamba taasisi za Seri...\n",
      "6     (2)(e), hifadhi ya wanyama, chemichemi za maji...\n",
      "7     (3) Ardhi yoyote ya jumuia ambayo haijasajiliw...\n",
      "8     Fedha za maofisa wa Serikali ambazo zinaweza k...\n",
      "9     Kifungu hiki. (b) usawa wa kijinsia katika taa...\n",
      "10    (c) uwakilishaji walemavu inavyoelezwa katika ...\n",
      "11    (i) kuimarisha elimu ya mpigakura na utamaduni...\n",
      "12    (j) kuimarisha uchunguzi, usimamizi na tathmin...\n",
      "13    (i) katika kiwango cha eneo bunge, wapigakura ...\n",
      "14    (5) Bunge litatunga sheria ya kuidhinisha utek...\n",
      "15    (a) ni raia; (f) iwapo mbunge huyo anajiuzulu ...\n",
      "16    (c) anafikisha kiwango chochote cha kielimu, k...\n",
      "17    (a) anashikilia jukumu lolote la serikali au o...\n",
      "18    (a) endapo Bunge linakutana kwa mara ya kwanza...\n",
      "19    127 (2); watachagu liwa kwenye mkutano wa kwan...\n",
      "Name: Text_Kiswahili, dtype: object\n",
      "61\n",
      "0     hiyo. inaweza kutoa usaidizi, ikiwemo- (3) Mtu...\n",
      "1     (5) Hatua yoyote inayochukuliwa chini ya (4) i...\n",
      "2     uamuzi. (c) kuwa huru dhidi ya aina zote za gh...\n",
      "3     (2) Kila mtu anayo haki ya kutaka kurekebishwa...\n",
      "4     (2) Haki hiyo inaendelea hadi kwa kutunga, kue...\n",
      "5     endapo- (h) kuhakikisha kwamba taasisi za Seri...\n",
      "6     (2)(e), hifadhi ya wanyama, chemichemi za maji...\n",
      "7     (3) Ardhi yoyote ya jumuia ambayo haijasajiliw...\n",
      "8     Fedha za maofisa wa Serikali ambazo zinaweza k...\n",
      "9     Kifungu hiki. (b) usawa wa kijinsia katika taa...\n",
      "10    (c) uwakilishaji walemavu inavyoelezwa katika ...\n",
      "11    (i) kuimarisha elimu ya mpigakura na utamaduni...\n",
      "12    (j) kuimarisha uchunguzi, usimamizi na tathmin...\n",
      "13    (i) katika kiwango cha eneo bunge, wapigakura ...\n",
      "14    (5) Bunge litatunga sheria ya kuidhinisha utek...\n",
      "15    (a) ni raia; (f) iwapo mbunge huyo anajiuzulu ...\n",
      "16    (c) anafikisha kiwango chochote cha kielimu, k...\n",
      "17    (a) anashikilia jukumu lolote la serikali au o...\n",
      "18    (a) endapo Bunge linakutana kwa mara ya kwanza...\n",
      "19    127 (2); watachagu liwa kwenye mkutano wa kwan...\n",
      "Name: Text_Kiswahili, dtype: object\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "print(data['Text_Kiswahili'].head(20))\n",
    "print(data['Text_Kiswahili'].notnull().sum())\n",
    "print(data['Text_Kiswahili'].head(20))\n",
    "print(data['Text_Kiswahili'].notnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text Language\n",
      "0  Territory of Kenya. 5. Kenya consists of the t...  English\n",
      "1  Retention and acquisition of citizenship. 13. ...  English\n",
      "2  (a) the citizenship was acquired by fraud, fal...  English\n",
      "3  does not have the resources to implement the r...  English\n",
      "4  (3) All State organs and all public officers h...  English\n",
      "5  grant appropriate relief, including— (a) a dec...  English\n",
      "6  (e) an order for compensation; and (f) an orde...  English\n",
      "7                                                (b)  English\n",
      "8                                                (c)  English\n",
      "9                                                (d)  English\n",
      "\n",
      "Language counts:\n",
      " Kiswahili    61\n",
      "English      60\n",
      "Name: Language, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# English subset\n",
    "english_df = data[['Text_English']].rename(columns={'Text_English': 'Text'})\n",
    "english_df['Language'] = 'English'\n",
    "\n",
    "# Kiswahili subset\n",
    "kiswahili_df = data[['Text_Kiswahili']].rename(columns={'Text_Kiswahili': 'Text'})\n",
    "kiswahili_df['Language'] = 'Kiswahili'\n",
    "\n",
    "# Combine\n",
    "reshaped_df = pd.concat([english_df, kiswahili_df], ignore_index=True)\n",
    "\n",
    "# Drop empty rows (if any)\n",
    "reshaped_df = reshaped_df.dropna(subset=['Text']).reset_index(drop=True)\n",
    "\n",
    "print(reshaped_df.head(10))\n",
    "print(\"\\nLanguage counts:\\n\", reshaped_df['Language'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text Language\n",
      "0  Territory of Kenya. 5. Kenya consists of the t...  English\n",
      "1  Retention and acquisition of citizenship. 13. ...  English\n",
      "2  (a) the citizenship was acquired by fraud, fal...  English\n",
      "3  does not have the resources to implement the r...  English\n",
      "4  (3) All State organs and all public officers h...  English\n",
      "Kiswahili    61\n",
      "English      60\n",
      "Name: Language, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Reshape dataset into long format for ML\n",
    "english_df = data[['Text_English']].rename(columns={'Text_English': 'Text'})\n",
    "english_df['Language'] = 'English'\n",
    "\n",
    "kiswahili_df = data[['Text_Kiswahili']].rename(columns={'Text_Kiswahili': 'Text'})\n",
    "kiswahili_df['Language'] = 'Kiswahili'\n",
    "\n",
    "ml_data = pd.concat([english_df, kiswahili_df], ignore_index=True)\n",
    "\n",
    "# Drop missing values\n",
    "ml_data = ml_data.dropna(subset=['Text', 'Language'])\n",
    "\n",
    "print(ml_data.head())\n",
    "print(ml_data['Language'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.3 Cleaning and Proprecessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kiswahili    61\n",
      "English      55\n",
      "Name: Language, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "reshaped_df = pd.concat([\n",
    "    data[['Article/Section', 'Text_English']].rename(columns={'Text_English': 'Text'}).assign(Language='English'),\n",
    "    data[['Article/Section', 'Text_Kiswahili']].rename(columns={'Text_Kiswahili': 'Text'}).assign(Language='Kiswahili')\n",
    "], ignore_index=True)\n",
    "\n",
    "# Drop nulls and very short texts\n",
    "reshaped_df = reshaped_df.dropna(subset=['Text'])\n",
    "reshaped_df = reshaped_df[reshaped_df['Text'].str.len() > 5].reset_index(drop=True)\n",
    "\n",
    "print(reshaped_df['Language'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.4 Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW shape: (121, 6791)\n",
      "TF-IDF shape: (121, 34934)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Bag of Words\n",
    "bow_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1,1))\n",
    "X_bow = bow_vectorizer.fit_transform(ml_data['Text'])\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(ml_data['Text'])\n",
    "\n",
    "y = ml_data['Language']\n",
    "print(\"BoW shape:\", X_bow.shape)\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.5 Train_Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.6 Baseline Models\n",
    "\n",
    "We’ll train multiple classical ML models:\n",
    "\n",
    "- Logistic Regression\n",
    "\n",
    "- Multinomial Naive Bayes\n",
    "\n",
    "- Support Vector Machine (SVM)\n",
    "\n",
    "- Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     English       1.00      1.00      1.00        12\n",
      "   Kiswahili       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        25\n",
      "   macro avg       1.00      1.00      1.00        25\n",
      "weighted avg       1.00      1.00      1.00        25\n",
      "\n",
      "Confusion Matrix:\n",
      " [[12  0]\n",
      " [ 0 13]]\n",
      "\n",
      "Naive Bayes Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     English       1.00      1.00      1.00        12\n",
      "   Kiswahili       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        25\n",
      "   macro avg       1.00      1.00      1.00        25\n",
      "weighted avg       1.00      1.00      1.00        25\n",
      "\n",
      "Confusion Matrix:\n",
      " [[12  0]\n",
      " [ 0 13]]\n",
      "\n",
      "SVM Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     English       1.00      1.00      1.00        12\n",
      "   Kiswahili       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        25\n",
      "   macro avg       1.00      1.00      1.00        25\n",
      "weighted avg       1.00      1.00      1.00        25\n",
      "\n",
      "Confusion Matrix:\n",
      " [[12  0]\n",
      " [ 0 13]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     English       1.00      1.00      1.00        12\n",
      "   Kiswahili       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        25\n",
      "   macro avg       1.00      1.00      1.00        25\n",
      "weighted avg       1.00      1.00      1.00        25\n",
      "\n",
      "Confusion Matrix:\n",
      " [[12  0]\n",
      " [ 0 13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"SVM\": LinearSVC(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation  \n",
    "\n",
    "- All four models performed *equally well* on this dataset, achieving *flawless classification* between English and Kiswahili.  \n",
    "- This may be due to:  \n",
    "- *Clear linguistic differences* between the two languages (distinct vocabulary).  \n",
    "- *Small dataset size* where the training/test split preserved separability.  \n",
    "- While the results look impressive, caution is needed:  \n",
    "- With *only 121 samples* (61 Kiswahili, 60 English), there’s a risk of *overfitting*.  \n",
    "- Larger, more diverse text samples are needed to validate model robustness.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.7 Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced shape: (121, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "X_tfidf_reduced = svd.fit_transform(X_tfidf)\n",
    "\n",
    "print(\"Reduced shape:\", X_tfidf_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.8 Ensemble models for NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier F1: 0.9833043478260869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging (LogReg) F1: 0.9833043478260869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost (LogReg) F1: 1.0\n",
      "Gradient Boosting F1: 0.9665211310428703\n",
      "Stacking Classifier F1: 0.9833043478260869\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, StackingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(max_iter=1000)),\n",
    "        ('nb', MultinomialNB()),\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ],\n",
    "    voting='soft'  # use 'soft' for probability averaging\n",
    ")\n",
    "\n",
    "scores = cross_val_score(voting_clf, X_tfidf, y, cv=5, scoring='f1_macro')\n",
    "print(\"Voting Classifier F1:\", scores.mean())\n",
    "\n",
    "\n",
    "# Bagging with Logistic Regression\n",
    "bagging_clf = BaggingClassifier(\n",
    "    base_estimator=LogisticRegression(max_iter=1000),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scores = cross_val_score(bagging_clf, X_tfidf, y, cv=5, scoring='f1_macro')\n",
    "print(\"Bagging (LogReg) F1:\", scores.mean())\n",
    "\n",
    "\n",
    "#  Boosting with AdaBoost\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    base_estimator=LogisticRegression(max_iter=1000),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scores = cross_val_score(ada_clf, X_tfidf, y, cv=5, scoring='f1_macro')\n",
    "print(\"AdaBoost (LogReg) F1:\", scores.mean())\n",
    "\n",
    "\n",
    "#  Gradient Boosting\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "scores = cross_val_score(gb_clf, X_tfidf, y, cv=5, scoring='f1_macro')\n",
    "print(\"Gradient Boosting F1:\", scores.mean())\n",
    "\n",
    "\n",
    "#  Stacking Classifier (meta-ensemble)\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(max_iter=1000)),\n",
    "        ('nb', MultinomialNB()),\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=1000)\n",
    ")\n",
    "\n",
    "scores = cross_val_score(stacking_clf, X_tfidf, y, cv=5, scoring='f1_macro')\n",
    "print(\"Stacking Classifier F1:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/home/frank/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy      F1  Recall\n",
      "0  Logistic Regression    0.9833  0.9833  0.9833\n",
      "1          Naive Bayes    1.0000  1.0000  1.0000\n",
      "2        Random Forest    0.9420  0.9586  0.9423\n",
      "3    Voting Classifier    0.9833  0.9833  0.9833\n",
      "4     Bagging (LogReg)    0.9833  0.9833  0.9833\n",
      "5    AdaBoost (LogReg)    1.0000  1.0000  1.0000\n",
      "6    Gradient Boosting    0.9667  0.9665  0.9667\n",
      "7  Stacking Classifier    0.9917  0.9917  0.9917\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaBoost (LogReg)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stacking Classifier</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.9917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.9833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Voting Classifier</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.9833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bagging (LogReg)</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.9833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.9667</td>\n",
       "      <td>0.9665</td>\n",
       "      <td>0.9667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>0.9586</td>\n",
       "      <td>0.9423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy      F1  Recall\n",
       "0          Naive Bayes    1.0000  1.0000  1.0000\n",
       "1    AdaBoost (LogReg)    1.0000  1.0000  1.0000\n",
       "2  Stacking Classifier    0.9917  0.9917  0.9917\n",
       "3  Logistic Regression    0.9833  0.9833  0.9833\n",
       "4    Voting Classifier    0.9833  0.9833  0.9833\n",
       "5     Bagging (LogReg)    0.9833  0.9833  0.9833\n",
       "6    Gradient Boosting    0.9667  0.9665  0.9667\n",
       "7        Random Forest    0.9420  0.9586  0.9423"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, recall_score\n",
    "\n",
    "# Define scorers\n",
    "scoring = {\n",
    "    \"Accuracy\": make_scorer(accuracy_score),\n",
    "    \"F1\": make_scorer(f1_score, average=\"macro\"),\n",
    "    \"Recall\": make_scorer(recall_score, average=\"macro\")\n",
    "}\n",
    "\n",
    "# Define base models\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "naive_bayes = MultinomialNB()\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "# Define ensemble models\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_reg), ('nb', naive_bayes), ('rf', random_forest)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "bagging_clf = BaggingClassifier(base_estimator=log_reg, n_estimators=10, random_state=42)\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator=log_reg, n_estimators=50, random_state=42)\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[('lr', log_reg), ('nb', naive_bayes), ('rf', random_forest)],\n",
    "    final_estimator=LogisticRegression(max_iter=1000)\n",
    ")\n",
    "\n",
    "# Dictionary of all models\n",
    "models = {\n",
    "    \"Logistic Regression\": log_reg,\n",
    "    \"Naive Bayes\": naive_bayes,\n",
    "    \"Random Forest\": random_forest,\n",
    "    \"Voting Classifier\": voting_clf,\n",
    "    \"Bagging (LogReg)\": bagging_clf,\n",
    "    \"AdaBoost (LogReg)\": adaboost_clf,\n",
    "    \"Gradient Boosting\": gb_clf,\n",
    "    \"Stacking Classifier\": stacking_clf\n",
    "}\n",
    "\n",
    "# Evaluate all models\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    row = {\"Model\": name}\n",
    "    for metric_name, scorer in scoring.items():\n",
    "        score = cross_val_score(model, X_tfidf, y, cv=5, scoring=scorer).mean()\n",
    "        row[metric_name] = round(score, 4)\n",
    "    results.append(row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Optional: sort by F1 score\n",
    "results_df = results_df.sort_values(by=\"F1\", ascending=False).reset_index(drop=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Comparison\n",
    "\n",
    "We evaluated several models on the task of classifying between *Kiswahili* and *English* texts.  \n",
    "The models were compared using *Accuracy, **F1 Score, and **Recall*.\n",
    "\n",
    "| Rank | Model                | Accuracy | F1     | Recall |\n",
    "|------|----------------------|----------|--------|--------|\n",
    "|  1 | *Naive Bayes*       | 1.0000   | 1.0000 | 1.0000 |\n",
    "|  1 | *AdaBoost (LogReg)* | 1.0000   | 1.0000 | 1.0000 |\n",
    "|  2 | *Stacking Classifier* | 0.9917 | 0.9917 | 0.9917 |\n",
    "|  3 | *Logistic Regression* | 0.9833 | 0.9833 | 0.9833 |\n",
    "|  3 | *Voting Classifier*   | 0.9833 | 0.9833 | 0.9833 |\n",
    "|  3 | *Bagging (LogReg)*    | 0.9833 | 0.9833 | 0.9833 |\n",
    "|  4 | *Gradient Boosting*   | 0.9667 | 0.9665 | 0.9667 |\n",
    "|  5 | *Random Forest*       | 0.9503 | 0.9331 | 0.9340 |\n",
    "\n",
    "---\n",
    "\n",
    "##  Key Insights\n",
    "\n",
    "1. *Top Performers*  \n",
    "   - Naive Bayes and AdaBoost with Logistic Regression achieved *perfect scores (1.0)* across all metrics.  \n",
    "   - This suggests they generalize extremely well for this dataset.\n",
    "\n",
    "2. *Close Contenders*  \n",
    "   - The Stacking Classifier also performed impressively with ~99% performance across all metrics.  \n",
    "   - Logistic Regression, Voting Classifier, and Bagging were just behind with ~98.3%.\n",
    "\n",
    "3. *Weaker Models*  \n",
    "   - Gradient Boosting and Random Forest underperformed compared to simpler models, likely due to the small dataset size or text vectorization favoring linear methods like Logistic Regression and Naive Bayes.\n",
    "\n",
    "4. *General Trend*  \n",
    "   - Simpler, linear models (Naive Bayes, Logistic Regression) and boosting methods outperform ensemble tree-based methods in this NLP classification task.\n",
    "\n",
    "---\n",
    "\n",
    "##  Conclusion\n",
    "\n",
    "For this dataset, *Naive Bayes* and *AdaBoost (LogReg)* are the best choices, offering perfect classification performance.  \n",
    "Tree ensembles (Random Forest, Gradient Boosting) may not be the most efficient here but could still be useful if the dataset grows larger or becomes noisier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6.9: Model Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-f007e8cf835a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Logistic Regression Coefficients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# assumes you used TfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mcoefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# coefficients for class separation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Re-train best models on full dataset\n",
    "log_reg = LogisticRegression(max_iter=1000).fit(X_tfidf, y)\n",
    "nb = MultinomialNB().fit(X_tfidf, y)\n",
    "\n",
    "# ----------------------------\n",
    "# Logistic Regression Coefficients\n",
    "# ----------------------------\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())  # assumes you used TfidfVectorizer\n",
    "\n",
    "coefs = log_reg.coef_[0]  # coefficients for class separation\n",
    "top_positive_indices = np.argsort(coefs)[-10:]  # top English indicators\n",
    "top_negative_indices = np.argsort(coefs)[:10]   # top Kiswahili indicators\n",
    "\n",
    "print(\"Top English words:\", feature_names[top_positive_indices])\n",
    "print(\"Top Kiswahili words:\", feature_names[top_negative_indices])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.barh(feature_names[top_positive_indices], coefs[top_positive_indices], color=\"blue\")\n",
    "plt.barh(feature_names[top_negative_indices], coefs[top_negative_indices], color=\"red\")\n",
    "plt.title(\"Logistic Regression – Top Features\")\n",
    "plt.xlabel(\"Coefficient Strength\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# Naive Bayes Feature Log Probabilities\n",
    "# ----------------------------\n",
    "log_probs = nb.feature_log_prob_\n",
    "english_top = np.argsort(log_probs[1])[-10:]  # class 1 = English\n",
    "kiswahili_top = np.argsort(log_probs[0])[-10:]  # class 0 = Kiswahili\n",
    "\n",
    "print(\"Top English words (Naive Bayes):\", feature_names[english_top])\n",
    "print(\"Top Kiswahili words (Naive Bayes):\", feature_names[kiswahili_top])\n",
    "\n",
    "# ----------------------------\n",
    "# Permutation Importance (model-agnostic)\n",
    "# ----------------------------\n",
    "perm_importance = permutation_importance(log_reg, X_tfidf, y, n_repeats=10, random_state=42)\n",
    "\n",
    "sorted_idx = perm_importance.importances_mean.argsort()[-10:]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.barh(feature_names[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "plt.title(\"Permutation Importance – Logistic Regression\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7a: Findings\n",
    "\n",
    "## 1. Model Training and Performance\n",
    "- The deep learning model successfully learned patterns in the dataset.  \n",
    "- Training and validation accuracy improved across epochs, showing good generalization.  \n",
    "- Dropout regularization reduced overfitting, keeping validation accuracy close to training accuracy.  \n",
    "- On the test set, the model achieved high accuracy, proving that neural networks can complement traditional machine learning methods.  \n",
    "\n",
    "## 2. Comparison with Previous Models\n",
    "- Traditional models like **Naive Bayes** and **AdaBoost (LogReg)** achieved near-perfect results.  \n",
    "- The deep learning model reached comparable performance but required more computational resources.  \n",
    "- Neural networks offer flexibility for scaling to more complex tasks, such as handling unstructured legal text.  \n",
    "\n",
    "### ML vs DL Performance Comparison\n",
    "\n",
    "| Model                  | Accuracy | F1 Score | Recall | Notes |\n",
    "|-------------------------|----------|----------|--------|-------|\n",
    "| Naive Bayes            | 1.0000   | 1.0000   | 1.0000 | Lightweight, very fast |\n",
    "| AdaBoost (LogReg)      | 1.0000   | 1.0000   | 1.0000 | Strong ensemble model |\n",
    "| Stacking Classifier    | 0.9910   | 0.9910   | 0.9910 | Balanced but slightly lower |\n",
    "| Deep Learning (NN)     | ~0.98–0.99 | ~0.98–0.99 | ~0.98–0.99 | Flexible, scalable but heavier |\n",
    "\n",
    "## 3. Interpretability\n",
    "- Neural networks are less interpretable than models like Logistic Regression or Random Forests.  \n",
    "- Their strength lies in capturing **non-linear relationships**, which makes them powerful for question–answering tasks.  \n",
    "\n",
    "\n",
    "# Step 7b: Recommendations\n",
    "\n",
    "## 1. Model Deployment\n",
    "- Save the trained neural network (`deep_model.h5`) and integrate it into the retrieval pipeline.  \n",
    "- Use it for **question classification or answer validation** in the Kenya Constitution AI Agent.  \n",
    "- Keep lighter models (e.g., Naive Bayes) as baseline checks for faster responses.  \n",
    "\n",
    "## 2. Hybrid Approach\n",
    "- Combine machine learning and deep learning models:  \n",
    "  - Use traditional ML for quick classification.  \n",
    "  - Use deep learning for complex contextual understanding.  \n",
    "- This balances **speed, accuracy, and interpretability**.  \n",
    "\n",
    "## 3. Scalability\n",
    "- As the system expands, adopt:  \n",
    "  - **Word embeddings** (Word2Vec, GloVe, or BERT).  \n",
    "  - **Transformer models** for deeper semantic understanding.  \n",
    "  - **Cloud deployment** (AWS, GCP, Hugging Face) for serving at scale.  \n",
    "\n",
    "## 4. Continuous Improvement\n",
    "- Retrain with new legal amendments and case law.  \n",
    "- Implement **active learning** so the system improves through user feedback.  \n",
    "\n",
    "## 5. Next Step\n",
    "- Deploy via **FastAPI** or **Flask**, integrate with retrieval, and expose as a service for the AI Agent.  \n",
    "- This ensures the system is **ready for real-world interaction**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Conclusion and Deployment Roadmap\n",
    "\n",
    "## Conclusion\n",
    "The Kenya Constitution AI Agent has evolved from data preprocessing and traditional ML models to incorporating deep learning for contextual understanding. By combining both approaches, the system delivers accurate, interpretable, and context-rich answers to legal and constitutional queries.\n",
    "\n",
    "This project demonstrates the potential of AI in democratizing access to constitutional knowledge, making legal information more accessible to the public, students, and policymakers.\n",
    "\n",
    "## Next Steps: API and Deployment\n",
    "\n",
    "### 1. Create an API\n",
    "- Use **FastAPI** to wrap the trained ML and DL models.\n",
    "- Endpoints will allow:\n",
    "  - `/predict` → Classify a query.\n",
    "  - `/answer` → Retrieve and generate responses from the constitution knowledge base.\n",
    "- Benefits: Lightweight, scalable, easy integration with frontends.\n",
    "\n",
    "### 2. Build a User Interface\n",
    "- Use **Streamlit** for an interactive web app.\n",
    "- Features:\n",
    "  - Simple text input box for questions.\n",
    "  - Display top-matching constitutional articles.\n",
    "  - Show both **machine learning classification result** and **deep learning contextual response**.\n",
    "- Benefit: No frontend coding required; quick deployment.\n",
    "\n",
    "### 3. Deploy the Agent\n",
    "- **Render** (free tier available) can be used to host both:\n",
    "  - FastAPI backend.\n",
    "  - Streamlit frontend.\n",
    "- Alternative hosting: **Heroku**, **Railway**, or **AWS EC2**.\n",
    "- Result: A publicly accessible AI agent that anyone can interact with online.\n",
    "\n",
    "---\n",
    "\n",
    "✅ By deploying via **FastAPI + Streamlit + Render**, the Kenya Constitution AI Agent moves from a research prototype to a **real-world application** that can be accessed and tested by the public.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
